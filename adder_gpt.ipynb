{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1eafde01-6d84-4514-9773-888012a03b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e17baa-7d71-452e-8b45-2052f0d671b4",
   "metadata": {},
   "source": [
    "# Vocabulary Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "12f80504-350e-4a16-a8b7-7cc6afbf268a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "itos = {\n",
    "    0: \"0\",\n",
    "    1: \"1\",\n",
    "    2: \"2\",\n",
    "    3: \"3\",\n",
    "    4: \"4\",\n",
    "    5: \"5\",\n",
    "    6: \"6\",\n",
    "    7: \"7\",\n",
    "    8: \"8\",\n",
    "    9: \"9\",\n",
    "    10: \"+\",\n",
    "    11: \"=\",\n",
    "    12: \"$\",\n",
    "    13: \".\"\n",
    "}\n",
    "\n",
    "stoi = {v: k for k,v in itos.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e9c6dc69-4135-461f-9b20-f83a4ca50ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdditionDataset(Dataset):\n",
    "    def __init__(self, random_seed = 42, max_num_units=6, num_elements = 20000):\n",
    "        self.random_seed = 42\n",
    "        self.num_elements = num_elements\n",
    "        self.max_num_units = max_num_units\n",
    "        self.max_len = max_num_units + 1 + max_num_units + 1 + (max_num_units+1) + 1\n",
    "        \n",
    "        \n",
    "    def select_random_max_val(self):\n",
    "        max_val = 1 * 10**self.max_num_units - 1\n",
    "        \n",
    "        prob = random.random()\n",
    "        \n",
    "        if prob < 0.02:\n",
    "            max_val = 10\n",
    "        \n",
    "        elif prob < 0.06:\n",
    "            max_val = 1e2\n",
    "        \n",
    "        elif prob < 0.11:\n",
    "            max_val = 1e3\n",
    "        \n",
    "        elif prob < 0.16:\n",
    "            max_val = 1e4\n",
    "        \n",
    "        elif prob < 0.20:\n",
    "            max_val = 1e5\n",
    "            \n",
    "        return max_val\n",
    "            \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # randomly select two integers, generate the string and return\n",
    "        a_max_val = self.select_random_max_val()\n",
    "        b_max_val = self.select_random_max_val()\n",
    "        \n",
    "        a = random.randint(0, a_max_val)\n",
    "        b = random.randint(0, b_max_val)\n",
    "        \n",
    "        answer = a + b\n",
    "        \n",
    "        equation_string = str(a) + \"+\" + str(b) + \"=\" + str(answer)[::-1] + \"$\"\n",
    "        while len(equation_string) < self.max_len:\n",
    "            equation_string += \".\"\n",
    "        \n",
    "        # print(equation_string)\n",
    "        \n",
    "        x_list = [stoi[c] for c in equation_string[0:-1]]\n",
    "        y_list = [stoi[c] for c in equation_string[1:]]\n",
    "        \n",
    "        mask_len = len(str(a) + \"+\" + str(b) + \"=\") - 1\n",
    "        \n",
    "        x,y = torch.tensor(x_list), torch.tensor(y_list)\n",
    "        y[:mask_len] = -1\n",
    "        y = torch.where(y == 13,-1, y)\n",
    "        return x, y\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2ae4ad18-33ed-4d27-8640-57e4d737ec70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8,  4,  1,  8,  9, 10,  5,  2, 11,  1,  4,  2,  4,  8, 12, 13, 13, 13,\n",
       "         13, 13, 13]),\n",
       " tensor([-1, -1, -1, -1, -1, -1, -1, -1,  1,  4,  2,  4,  8, 12, -1, -1, -1, -1,\n",
       "         -1, -1, -1]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = AdditionDataset()\n",
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73ee0e-3ba5-46b4-93dc-8712fea319fc",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4ef62b09-2d9c-4c5c-90a5-51bf9b19ab86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.my_weight = nn.Parameter(torch.ones(config.n_embd))\n",
    "        self.my_bias = nn.Parameter(torch.zeros(config.n_embd)) if config.use_bias else None\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, \n",
    "                     normalized_shape=self.my_weight.shape, \n",
    "                     weight=self.my_weight, \n",
    "                     bias=self.my_bias, \n",
    "                     eps=1e-5)\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.use_bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.use_bias)\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.head_size = int(config.n_embd / config.n_head) # h_dim\n",
    "        self.num_head = config.n_head # number of heads\n",
    "        self.block_size = config.block_size\n",
    "        \n",
    "        self.register_buffer(\"att_mask\", torch.triu(float('-inf') * torch.ones(config.block_size, config.block_size), diagonal=1).view(1, 1, self.block_size, self.block_size))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, t, c = x.size()\n",
    "        \n",
    "        q, k, v = self.c_attn(x).split(c, dim=-1) # (b,t,c)\n",
    "        \n",
    "        q = q.view(b, t, self.num_head, self.head_size).transpose(2,1) # (b, n_h, t, h_dim)\n",
    "        k = k.view(b, t, self.num_head, self.head_size).transpose(2,1) # (b, n_h, t, h_dim)\n",
    "        v = v.view(b, t, self.num_head, self.head_size).transpose(2,1) # (b, n_h, t, h_dim)\n",
    "        \n",
    "        att_scores = q @ k.transpose(-1, -2) / math.sqrt(self.head_size) # (b, n_h, t, h_dim) @ (b, n_h, h_dim, t) --> (b, n_h, t, t)\n",
    "        \n",
    "        # mask the scores\n",
    "        att_scores += self.att_mask[:, :, :t, :t]\n",
    "        \n",
    "        att_scores = torch.softmax(att_scores, dim=-1) # perform softmax for each element\n",
    "        \n",
    "        # perform dropout\n",
    "        att_scores = self.attn_dropout(att_scores) #(b, n_h, t, t)\n",
    "        \n",
    "        out = att_scores @ v # (b, n_h, t, t) @ (b, b_h, t, h_dim) --> (b, n_h, t, h_dim)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous() # (b, t, n_h, h_dim)\n",
    "        out = out.view(b, t, c)\n",
    "        out = self.resid_dropout(self.c_proj(out))\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.up_proj = nn.Linear(config.n_embd, 4*config.n_embd, bias=config.use_bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.down_proj = nn.Linear(config.n_embd * 4, config.n_embd, bias=config.use_bias)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.gelu(self.up_proj(x))\n",
    "        out = self.down_proj(self.dropout(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln_1 = LayerNorm(config)\n",
    "        self.sa = SelfAttention(config)\n",
    "        \n",
    "        self.ln_2 = LayerNorm(config)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x +  self.sa(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.layers = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        \n",
    "        self.ln_f = LayerNorm(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=config.use_bias)\n",
    "        # self.lm_head.weight = self.wte.weight\n",
    "    \n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # make sure x is on same device\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        idx = idx.to(device)\n",
    "        \n",
    "        b, t = idx.size()\n",
    "        \n",
    "        pos = torch.arange(0, t, device=device, dtype=torch.long)\n",
    "        \n",
    "        tok_emb = self.wte(idx) # (b, t, c)\n",
    "        pos_emb = self.wpe(pos) # (t,) --> (t, c)\n",
    "        \n",
    "        x = tok_emb + pos_emb # (b, t, c)\n",
    "        \n",
    "        x = self.drop(x)\n",
    "                \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        \n",
    "        logits = self.lm_head(x) # (b,t, c) --> (b, t, vocab_size)\n",
    "        \n",
    "        loss = None\n",
    "        # compute cross entropy loss if targets are provided\n",
    "        if targets is not None: # targets will be provided in shape b, t\n",
    "            unrolled_targets = targets.view(b*t)\n",
    "            unrolled_logits = logits.view(b*t, -1)\n",
    "            \n",
    "            loss = F.cross_entropy(unrolled_logits, unrolled_targets, ignore_index=-1)        \n",
    "                \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b7613-62ec-45a3-9698-01aa1d3f8473",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "abc292b9-8d4a-4abb-a237-dc9637f3686c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_loop(model, train_loader, eval_loader, optimizer, scheduler, num_epochs=10, eval_every_iter=1000, print_every=200):\n",
    "    num_iter = 0\n",
    "    \n",
    "    training_log = {\n",
    "        'num_iter': [],\n",
    "        'training_loss': [],\n",
    "        'training_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    \n",
    "    for epoch_num in range(num_epochs):\n",
    "        for i, (x,y) in enumerate(train_loader):\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            outputs, loss = model(x, targets=y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % print_every == 0:\n",
    "                print(f\"Epoch {epoch_num}, iteration {i}: Training Loss: {loss.item()}\")\n",
    "                \n",
    "            if num_iter % eval_every_iter == 0:\n",
    "                \n",
    "                training_log['num_iter'].append(num_iter)\n",
    "                training_log['training_loss'].append(loss.item())\n",
    "                \n",
    "                val_loss, val_acc = run_eval(model, eval_loader)\n",
    "                print(f\"Epoch {epoch_num}, iteration {i}: Val loss: {val_loss}, val acc: {val_acc}\")\n",
    "                train_acc = calculate_accuracy(outputs, y)\n",
    "                \n",
    "                training_log['training_acc'].append(train_acc)\n",
    "                training_log['val_loss'].append(val_loss)\n",
    "                training_log['val_acc'].append(val_acc)\n",
    "                \n",
    "                save_embeddings(model, num_iter)\n",
    "\n",
    "            num_iter += 1\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    return training_log\n",
    "\n",
    "            \n",
    "def save_embeddings(model, num_iter):\n",
    "    device = next(model.parameters()).device\n",
    "    idx = torch.arange(0, 10, device=device)\n",
    "    embeddings = model.wte(idx)\n",
    "    \n",
    "    path = Path(\"embeddings/\")\n",
    "    if not path.exists():\n",
    "        path.mkdir()\n",
    "        \n",
    "    torch.save(embeddings, f\"embeddings/{num_iter}.pt\")\n",
    "    return embeddings\n",
    "    \n",
    "\n",
    "def calculate_accuracy(pred, yb):\n",
    "    # pred shape: (b, t, v)\n",
    "    _, pred_idx, = pred.max(dim=-1) # (b, t, v) --> (b, t)\n",
    "    \n",
    "    correct = 0\n",
    "    total = pred_idx.size(0)\n",
    "    \n",
    "    # yb shape: (b, t)\n",
    "    for i in range(pred.size(0)):\n",
    "        pred_row = pred_idx[i].tolist()\n",
    "        label_row = yb[i].tolist()\n",
    "        \n",
    "        \n",
    "        # truncate all positions in front with -1\n",
    "        first_index = 0\n",
    "        for i in range(len(label_row)):\n",
    "            if label_row[i] != -1:\n",
    "                first_index = i\n",
    "                break\n",
    "        \n",
    "        stop_index = label_row.index(12)\n",
    "        \n",
    "        pred_row = pred_row[first_index: stop_index]\n",
    "        label_row = label_row[first_index: stop_index]\n",
    "        \n",
    "        if pred_row == label_row:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / total\n",
    "    # truncate all positions in end after (and including) $\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_eval(model, loader):\n",
    "    # calculate loss and also the percentage correct!\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    num_iter = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i, (x,y) in enumerate(loader):\n",
    "        device = next(model.parameters()).device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        out, loss = model(x, targets=y)\n",
    "        acc = calculate_accuracy(out, y)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "        num_iter += 1\n",
    "    \n",
    "    total_loss /= num_iter\n",
    "    total_acc /= num_iter\n",
    "    model.train()\n",
    "    \n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "342a5d6c-a374-430f-a8fe-c6aabf5ebe6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, iteration 0: Training Loss: 3.1260454654693604\n",
      "Epoch 0, iteration 0: Val loss: 2.7471307441592216, val acc: 0.0\n",
      "Epoch 0, iteration 200: Training Loss: 1.9223836660385132\n",
      "Epoch 0, iteration 400: Training Loss: 1.8935703039169312\n",
      "Epoch 0, iteration 500: Val loss: 1.8774835914373398, val acc: 0.0\n",
      "Epoch 0, iteration 600: Training Loss: 1.9185903072357178\n",
      "Epoch 1, iteration 0: Training Loss: 1.8560949563980103\n",
      "Epoch 1, iteration 200: Training Loss: 1.8319061994552612\n",
      "Epoch 1, iteration 375: Val loss: 1.6616678461432457, val acc: 0.0\n",
      "Epoch 1, iteration 400: Training Loss: 1.6441210508346558\n",
      "Epoch 1, iteration 600: Training Loss: 1.7708826065063477\n",
      "Epoch 2, iteration 0: Training Loss: 1.605664610862732\n",
      "Epoch 2, iteration 200: Training Loss: 1.669075608253479\n",
      "Epoch 2, iteration 250: Val loss: 1.6125296503305435, val acc: 0.0029296875\n",
      "Epoch 2, iteration 400: Training Loss: 1.6881994009017944\n",
      "Epoch 2, iteration 600: Training Loss: 1.608519196510315\n",
      "Epoch 3, iteration 0: Training Loss: 1.5864083766937256\n",
      "Epoch 3, iteration 125: Val loss: 1.3767340555787086, val acc: 0.0048828125\n",
      "Epoch 3, iteration 200: Training Loss: 1.3522869348526\n",
      "Epoch 3, iteration 400: Training Loss: 0.8958994746208191\n",
      "Epoch 3, iteration 600: Training Loss: 0.3931160867214203\n",
      "Epoch 4, iteration 0: Training Loss: 0.45428529381752014\n",
      "Epoch 4, iteration 0: Val loss: 0.290277776774019, val acc: 0.59765625\n",
      "Epoch 4, iteration 200: Training Loss: 0.3076433837413788\n",
      "Epoch 4, iteration 400: Training Loss: 0.19983534514904022\n",
      "Epoch 4, iteration 500: Val loss: 0.05373563575267326, val acc: 0.9189453125\n",
      "Epoch 4, iteration 600: Training Loss: 0.28611600399017334\n",
      "Epoch 5, iteration 0: Training Loss: 0.16750454902648926\n",
      "Epoch 5, iteration 200: Training Loss: 0.046602196991443634\n",
      "Epoch 5, iteration 375: Val loss: 0.03208823197201127, val acc: 0.947265625\n",
      "Epoch 5, iteration 400: Training Loss: 0.14464832842350006\n",
      "Epoch 5, iteration 600: Training Loss: 0.10361753404140472\n",
      "Epoch 6, iteration 0: Training Loss: 0.10903854668140411\n",
      "Epoch 6, iteration 200: Training Loss: 0.11931745707988739\n",
      "Epoch 6, iteration 250: Val loss: 0.023554708088340703, val acc: 0.9599609375\n",
      "Epoch 6, iteration 400: Training Loss: 0.04192977026104927\n",
      "Epoch 6, iteration 600: Training Loss: 0.035553935915231705\n",
      "Epoch 7, iteration 0: Training Loss: 0.08132215589284897\n",
      "Epoch 7, iteration 125: Val loss: 0.01918594186008704, val acc: 0.9638671875\n",
      "Epoch 7, iteration 200: Training Loss: 0.11015080660581589\n",
      "Epoch 7, iteration 400: Training Loss: 0.08149367570877075\n",
      "Epoch 7, iteration 600: Training Loss: 0.0711703822016716\n",
      "Epoch 8, iteration 0: Training Loss: 0.02388845942914486\n",
      "Epoch 8, iteration 0: Val loss: 0.009785402448642344, val acc: 0.9814453125\n",
      "Epoch 8, iteration 200: Training Loss: 0.02951517142355442\n",
      "Epoch 8, iteration 400: Training Loss: 0.01516207680106163\n",
      "Epoch 8, iteration 500: Val loss: 0.008903616959742067, val acc: 0.98046875\n",
      "Epoch 8, iteration 600: Training Loss: 0.11850135773420334\n",
      "Epoch 9, iteration 0: Training Loss: 0.010974946431815624\n",
      "Epoch 9, iteration 200: Training Loss: 0.042300451546907425\n",
      "Epoch 9, iteration 375: Val loss: 0.006971133031584031, val acc: 0.9853515625\n",
      "Epoch 9, iteration 400: Training Loss: 0.0433763787150383\n",
      "Epoch 9, iteration 600: Training Loss: 0.046092689037323\n",
      "Epoch 10, iteration 0: Training Loss: 0.05146658420562744\n",
      "Epoch 10, iteration 200: Training Loss: 0.02184896729886532\n",
      "Epoch 10, iteration 250: Val loss: 0.0038078202324243193, val acc: 0.9912109375\n",
      "Epoch 10, iteration 400: Training Loss: 0.018374569714069366\n",
      "Epoch 10, iteration 600: Training Loss: 0.024270979687571526\n",
      "Epoch 11, iteration 0: Training Loss: 0.012838862836360931\n",
      "Epoch 11, iteration 125: Val loss: 0.0031771616846754114, val acc: 0.9921875\n",
      "Epoch 11, iteration 200: Training Loss: 0.0354316309094429\n",
      "Epoch 11, iteration 400: Training Loss: 0.0006458965362980962\n",
      "Epoch 11, iteration 600: Training Loss: 0.005033685360103846\n",
      "Epoch 12, iteration 0: Training Loss: 0.017716653645038605\n",
      "Epoch 12, iteration 0: Val loss: 0.003350113802866872, val acc: 0.9921875\n",
      "Epoch 12, iteration 200: Training Loss: 0.012809486128389835\n",
      "Epoch 12, iteration 400: Training Loss: 0.02582138404250145\n",
      "Epoch 12, iteration 500: Val loss: 0.0022232200351766096, val acc: 0.994140625\n",
      "Epoch 12, iteration 600: Training Loss: 0.006310159340500832\n",
      "Epoch 13, iteration 0: Training Loss: 0.012968776747584343\n",
      "Epoch 13, iteration 200: Training Loss: 0.04991316422820091\n",
      "Epoch 13, iteration 375: Val loss: 0.003171132312473901, val acc: 0.9921875\n",
      "Epoch 13, iteration 400: Training Loss: 0.0007095859618857503\n",
      "Epoch 13, iteration 600: Training Loss: 0.008903414011001587\n",
      "Epoch 14, iteration 0: Training Loss: 0.007286937441676855\n",
      "Epoch 14, iteration 200: Training Loss: 0.01275706011801958\n",
      "Epoch 14, iteration 250: Val loss: 0.0012026561519746792, val acc: 0.9970703125\n",
      "Epoch 14, iteration 400: Training Loss: 0.025235615670681\n",
      "Epoch 14, iteration 600: Training Loss: 0.015247952193021774\n",
      "Epoch 15, iteration 0: Training Loss: 0.024096675217151642\n",
      "Epoch 15, iteration 125: Val loss: 0.0033627493822052656, val acc: 0.9921875\n",
      "Epoch 15, iteration 200: Training Loss: 0.001728396862745285\n",
      "Epoch 15, iteration 400: Training Loss: 0.009278574027121067\n",
      "Epoch 15, iteration 600: Training Loss: 0.002274461090564728\n",
      "Epoch 16, iteration 0: Training Loss: 0.002136779949069023\n",
      "Epoch 16, iteration 0: Val loss: 0.0028747384400276133, val acc: 0.9931640625\n",
      "Epoch 16, iteration 200: Training Loss: 0.00496349623426795\n",
      "Epoch 16, iteration 400: Training Loss: 0.013744124211370945\n",
      "Epoch 16, iteration 500: Val loss: 0.0019866636291396844, val acc: 0.9951171875\n",
      "Epoch 16, iteration 600: Training Loss: 0.0070998454466462135\n",
      "Epoch 17, iteration 0: Training Loss: 0.006269027013331652\n",
      "Epoch 17, iteration 200: Training Loss: 0.0031279034446924925\n",
      "Epoch 17, iteration 375: Val loss: 0.004027652984362362, val acc: 0.990234375\n",
      "Epoch 17, iteration 400: Training Loss: 0.004101669415831566\n",
      "Epoch 17, iteration 600: Training Loss: 0.0019322490552440286\n",
      "Epoch 18, iteration 0: Training Loss: 0.005029356572777033\n",
      "Epoch 18, iteration 200: Training Loss: 0.011432960629463196\n",
      "Epoch 18, iteration 250: Val loss: 0.002432863033902777, val acc: 0.9931640625\n",
      "Epoch 18, iteration 400: Training Loss: 0.0010537016205489635\n",
      "Epoch 18, iteration 600: Training Loss: 0.03035985492169857\n",
      "Epoch 19, iteration 0: Training Loss: 0.00393661530688405\n",
      "Epoch 19, iteration 125: Val loss: 0.00220927214895994, val acc: 0.9951171875\n",
      "Epoch 19, iteration 200: Training Loss: 0.028054969385266304\n",
      "Epoch 19, iteration 400: Training Loss: 0.0052072894759476185\n",
      "Epoch 19, iteration 600: Training Loss: 0.0376693494617939\n",
      "Epoch 20, iteration 0: Training Loss: 0.03337908163666725\n",
      "Epoch 20, iteration 0: Val loss: 0.0010369569223342978, val acc: 0.998046875\n",
      "Epoch 20, iteration 200: Training Loss: 0.0004190032195765525\n",
      "Epoch 20, iteration 400: Training Loss: 0.00047226977767422795\n",
      "Epoch 20, iteration 500: Val loss: 0.0008774500357588977, val acc: 0.998046875\n",
      "Epoch 20, iteration 600: Training Loss: 0.027312999591231346\n",
      "Epoch 21, iteration 0: Training Loss: 0.014335106126964092\n",
      "Epoch 21, iteration 200: Training Loss: 0.01573561504483223\n",
      "Epoch 21, iteration 375: Val loss: 0.0014717373542474377, val acc: 0.99609375\n",
      "Epoch 21, iteration 400: Training Loss: 0.0014152370858937502\n",
      "Epoch 21, iteration 600: Training Loss: 0.0018936832202598453\n",
      "Epoch 22, iteration 0: Training Loss: 0.0018541711615398526\n",
      "Epoch 22, iteration 200: Training Loss: 0.001193797797895968\n",
      "Epoch 22, iteration 250: Val loss: 0.0010772444285436222, val acc: 0.998046875\n",
      "Epoch 22, iteration 400: Training Loss: 0.002009411808103323\n",
      "Epoch 22, iteration 600: Training Loss: 0.0006003373418934643\n"
     ]
    }
   ],
   "source": [
    "class GPTConfig:\n",
    "    vocab_size = 14\n",
    "    block_size = 22\n",
    "    n_embd = 64\n",
    "    n_head = 8\n",
    "    n_layer = 6\n",
    "    use_bias = True\n",
    "    dropout = 0.1\n",
    "\n",
    "\n",
    "config = GPTConfig()\n",
    "model = GPT(config)\n",
    "model = model.to('cuda')\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "train_loader = DataLoader(AdditionDataset(), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(AdditionDataset(num_elements=1000), batch_size=32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.4)\n",
    "\n",
    "training_log = training_loop(model, train_loader, val_loader, optimizer, scheduler, eval_every_iter=500, num_epochs=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8410d4b3-07f7-48a8-9f57-9244fce4bad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"adder_gpt_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "14c53592-91d2-4364-9d8d-c1abd44bbc59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 21])\n",
      "torch.Size([32, 21])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb , yb = next(iter(train_loader))\n",
    "xb = xb.to('cuda')\n",
    "yb = yb.to('cuda')\n",
    "\n",
    "\n",
    "print(xb.shape)\n",
    "print(yb.shape)\n",
    "\n",
    "logits, loss = model(xb, targets=yb)\n",
    "# print(logits.size())\n",
    "# print(loss)\n",
    "\n",
    "\n",
    "calculate_accuracy(logits, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9d07eb4f-1a93-49c0-aa8d-a809881171f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dump() missing required argument 'file' (pos 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_log.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: dump() missing required argument 'file' (pos 2)"
     ]
    }
   ],
   "source": [
    "with open('training_log.pkl', 'wb') as f:\n",
    "    pickle.dump(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd87ee-f947-4c39-955b-6d39518c703b",
   "metadata": {},
   "source": [
    "# Messing around with LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f7a04d1-e9b2-4e34-b4d9-a4b1e4261912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.size(): torch.Size([1, 768])\n",
      "answer.size(): torch.Size([1, 768])\n",
      "tensor([-0.0444])\n",
      "tensor([1.0134])\n",
      "tensor([-0.6823,  0.9493, -0.2643, -0.6182, -0.0727,  1.3347, -1.3283,  1.2171,\n",
      "        -1.8012, -0.3760])\n",
      "tensor([-0.6296,  0.9806, -0.2170, -0.5662, -0.0279,  1.3609, -1.2670,  1.2448,\n",
      "        -1.7336, -0.3273])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 768)\n",
    "weight = torch.ones(1, 768)\n",
    "bias = torch.zeros(1, 768)\n",
    "\n",
    "answer = F.layer_norm(a, a.size(), weight, bias)\n",
    "\n",
    "print(f\"a.size(): {a.size()}\")\n",
    "print(f\"answer.size(): {answer.size()}\")\n",
    "\n",
    "a_avg = a.mean(dim=-1)\n",
    "a_std = a.std(dim=-1)\n",
    "\n",
    "print(a_avg)\n",
    "print(a_std)\n",
    "\n",
    "b = (a - a_avg) / a_std\n",
    "\n",
    "print(a[0, 0:10])\n",
    "print(b[0, 0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5309c02-559d-4028-8cfa-124ed98624af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.size(): torch.Size([2, 768])\n",
      "answer.size(): torch.Size([2, 768])\n",
      "tensor([-0.7745, -0.6437,  1.1870,  0.5638, -2.0744,  0.2115,  0.8607,  1.6832,\n",
      "         1.0694,  1.6970]) tensor([-0.7740, -0.6433,  1.1862,  0.5634, -2.0730,  0.2114,  0.8601,  1.6821,\n",
      "         1.0687,  1.6959])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 768)\n",
    "weight = torch.ones(768)\n",
    "bias = torch.zeros(768)\n",
    "\n",
    "answer = F.layer_norm(a, (a.size(-1),), weight, bias)\n",
    "\n",
    "print(f\"a.size(): {a.size()}\")\n",
    "print(f\"answer.size(): {answer.size()}\")\n",
    "\n",
    "a_avg = a.mean(dim=-1, keepdim=True)\n",
    "a_std = a.std(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "# print(f\"a.size(): {a.size()}\")\n",
    "# print(f\"a_avg.size(): {a_avg.size()}\")\n",
    "\n",
    "# a_avg = a_avg.view(a_avg.size(0), -1)\n",
    "# a_std = a_\n",
    "# print(a_avg.size())\n",
    "\n",
    "# print(a_std)\n",
    "\n",
    "b = (a - a_avg.view(a_avg.size(), -1)) / (a_std.view(a_std.size(), -1)**2 + 1e-5)**0.5\n",
    "\n",
    "print(answer[0, :10], b[0, :10])\n",
    "\n",
    "# print(a[0, 0:10])\n",
    "# print(b[0, 0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
