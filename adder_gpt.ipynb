{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1eafde01-6d84-4514-9773-888012a03b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e17baa-7d71-452e-8b45-2052f0d671b4",
   "metadata": {},
   "source": [
    "# Vocabulary Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "12f80504-350e-4a16-a8b7-7cc6afbf268a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "itos = {\n",
    "    0: \"0\",\n",
    "    1: \"1\",\n",
    "    2: \"2\",\n",
    "    3: \"3\",\n",
    "    4: \"4\",\n",
    "    5: \"5\",\n",
    "    6: \"6\",\n",
    "    7: \"7\",\n",
    "    8: \"8\",\n",
    "    9: \"9\",\n",
    "    10: \"+\",\n",
    "    11: \"=\",\n",
    "    12: \"$\",\n",
    "    13: \".\"\n",
    "}\n",
    "\n",
    "stoi = {v: k for k,v in itos.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e9c6dc69-4135-461f-9b20-f83a4ca50ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdditionDataset(Dataset):\n",
    "    def __init__(self, random_seed = 42, max_num_units=6, num_elements = 20000):\n",
    "        self.random_seed = 42\n",
    "        self.num_elements = num_elements\n",
    "        self.max_num_units = max_num_units\n",
    "        self.max_len = max_num_units + 1 + max_num_units + 1 + (max_num_units+1) + 1\n",
    "        \n",
    "        \n",
    "    def select_random_max_val(self):\n",
    "        max_val = 1 * 10**self.max_num_units - 1\n",
    "        \n",
    "        prob = random.random()\n",
    "        \n",
    "        if prob < 0.02:\n",
    "            max_val = 10\n",
    "        \n",
    "        elif prob < 0.06:\n",
    "            max_val = 1e2\n",
    "        \n",
    "        elif prob < 0.11:\n",
    "            max_val = 1e3\n",
    "        \n",
    "        elif prob < 0.16:\n",
    "            max_val = 1e4\n",
    "        \n",
    "        elif prob < 0.20:\n",
    "            max_val = 1e5\n",
    "            \n",
    "        return max_val\n",
    "            \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # randomly select two integers, generate the string and return\n",
    "        a_max_val = self.select_random_max_val()\n",
    "        b_max_val = self.select_random_max_val()\n",
    "        \n",
    "        a = random.randint(0, a_max_val)\n",
    "        b = random.randint(0, b_max_val)\n",
    "        \n",
    "        answer = a + b\n",
    "        \n",
    "        equation_string = str(a) + \"+\" + str(b) + \"=\" + str(answer)[::-1] + \"$\"\n",
    "        while len(equation_string) < self.max_len:\n",
    "            equation_string += \".\"\n",
    "        \n",
    "        # print(equation_string)\n",
    "        \n",
    "        x_list = [stoi[c] for c in equation_string[0:-1]]\n",
    "        y_list = [stoi[c] for c in equation_string[1:]]\n",
    "        \n",
    "        mask_len = len(str(a) + \"+\" + str(b) + \"=\") - 1\n",
    "        \n",
    "        x,y = torch.tensor(x_list), torch.tensor(y_list)\n",
    "        y[:mask_len] = -1\n",
    "        y = torch.where(y == 13,-1, y)\n",
    "        return x, y\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2ae4ad18-33ed-4d27-8640-57e4d737ec70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6,  4,  8,  9,  4, 10,  6,  5,  9,  6,  2,  5, 11,  9,  1,  5,  4,  2,\n",
       "          7, 12, 13]),\n",
       " tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  9,  1,  5,  4,  2,  7,\n",
       "         12, -1, -1]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = AdditionDataset()\n",
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73ee0e-3ba5-46b4-93dc-8712fea319fc",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ef62b09-2d9c-4c5c-90a5-51bf9b19ab86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.my_weight = nn.Parameter(torch.ones(config.n_embd))\n",
    "        self.my_bias = nn.Parameter(torch.zeros(config.n_embd)) if config.use_bias else None\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, \n",
    "                     normalized_shape=self.my_weight.shape, \n",
    "                     weight=self.my_weight, \n",
    "                     bias=self.my_bias, \n",
    "                     eps=1e-5)\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.use_bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.use_bias)\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.head_size = int(config.n_embd / config.n_head) # h_dim\n",
    "        self.num_head = config.n_head # number of heads\n",
    "        self.block_size = config.block_size\n",
    "        \n",
    "        self.register_buffer(\"att_mask\", torch.triu(float('-inf') * torch.ones(config.block_size, config.block_size), diagonal=1).view(1, 1, self.block_size, self.block_size))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, t, c = x.size()\n",
    "        \n",
    "        q, k, v = self.c_attn(x).split(c, dim=-1) # (b,t,c)\n",
    "        \n",
    "        q = q.view(b, t, self.num_head, self.head_size).transpose(2,1) # (b, n_h, t, h_dim)\n",
    "        k = k.view(b, t, self.num_head, self.head_size).transpose(2,1) # (b, n_h, t, h_dim)\n",
    "        v = v.view(b, t, self.num_head, self.head_size).transpose(2,1) # (b, n_h, t, h_dim)\n",
    "        \n",
    "        att_scores = q @ k.transpose(-1, -2) / math.sqrt(self.head_size) # (b, n_h, t, h_dim) @ (b, n_h, h_dim, t) --> (b, n_h, t, t)\n",
    "        \n",
    "        # mask the scores\n",
    "        att_scores += self.att_mask[:, :, :t, :t]\n",
    "        \n",
    "        att_scores = torch.softmax(att_scores, dim=-1) # perform softmax for each element\n",
    "        \n",
    "        # perform dropout\n",
    "        att_scores = self.attn_dropout(att_scores) #(b, n_h, t, t)\n",
    "        \n",
    "        out = att_scores @ v # (b, n_h, t, t) @ (b, b_h, t, h_dim) --> (b, n_h, t, h_dim)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous() # (b, t, n_h, h_dim)\n",
    "        out = out.view(b, t, c)\n",
    "        out = self.resid_dropout(self.c_proj(out))\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.up_proj = nn.Linear(config.n_embd, 4*config.n_embd, bias=config.use_bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.down_proj = nn.Linear(config.n_embd * 4, config.n_embd, bias=config.use_bias)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.gelu(self.up_proj(x))\n",
    "        out = self.down_proj(self.dropout(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln_1 = LayerNorm(config)\n",
    "        self.sa = SelfAttention(config)\n",
    "        \n",
    "        self.ln_2 = LayerNorm(config)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x +  self.sa(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.layers = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        \n",
    "        self.ln_f = LayerNorm(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=config.use_bias)\n",
    "        # self.lm_head.weight = self.wte.weight\n",
    "    \n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # make sure x is on same device\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        idx = idx.to(device)\n",
    "        \n",
    "        b, t = idx.size()\n",
    "        \n",
    "        pos = torch.arange(0, t, device=device, dtype=torch.long)\n",
    "        \n",
    "        tok_emb = self.wte(idx) # (b, t, c)\n",
    "        pos_emb = self.wpe(pos) # (t,) --> (t, c)\n",
    "        \n",
    "        x = tok_emb + pos_emb # (b, t, c)\n",
    "        \n",
    "        x = self.drop(x)\n",
    "                \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        \n",
    "        logits = self.lm_head(x) # (b,t, c) --> (b, t, vocab_size)\n",
    "        \n",
    "        loss = None\n",
    "        # compute cross entropy loss if targets are provided\n",
    "        if targets is not None: # targets will be provided in shape b, t\n",
    "            unrolled_targets = targets.view(b*t)\n",
    "            unrolled_logits = logits.view(b*t, -1)\n",
    "            \n",
    "            loss = F.cross_entropy(unrolled_logits, unrolled_targets, ignore_index=-1)        \n",
    "                \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b7613-62ec-45a3-9698-01aa1d3f8473",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "abc292b9-8d4a-4abb-a237-dc9637f3686c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_loop(model, train_loader, eval_loader, optimizer, scheduler, training_log, num_epochs=10, eval_every_iter=1000, print_every=200):\n",
    "    num_iter = 0\n",
    "    \n",
    "    training_log = {\n",
    "        'num_iter': [],\n",
    "        'training_loss': [],\n",
    "        'training_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    \n",
    "    for epoch_num in range(num_epochs):\n",
    "        for i, (x,y) in enumerate(train_loader):\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            outputs, loss = model(x, targets=y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % print_every == 0:\n",
    "                print(f\"Epoch {epoch_num}, iteration {i}: Training Loss: {loss.item()}\")\n",
    "                \n",
    "            if num_iter % eval_every_iter == 0:\n",
    "                \n",
    "                training_log['num_iter'].append(num_iter)\n",
    "                training_log['training_loss'].append(loss.item())\n",
    "                \n",
    "                val_loss, val_acc = run_eval(model, eval_loader)\n",
    "                print(f\"Epoch {epoch_num}, iteration {i}: Val loss: {val_loss}, val acc: {val_acc}\")\n",
    "                train_acc = calculate_accuracy(outputs, y)\n",
    "                \n",
    "                training_log['training_acc'].append(train_acc)\n",
    "                training_log['val_loss'].append(val_loss)\n",
    "                training_log['val_acc'].append(val_acc)\n",
    "                \n",
    "                save_embeddings(model, num_iter)\n",
    "\n",
    "            num_iter += 1\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    return training_log\n",
    "\n",
    "            \n",
    "def save_embeddings(model, num_iter):\n",
    "    device = next(model.parameters()).device\n",
    "    idx = torch.arange(0, 10, device=device)\n",
    "    embeddings = model.wte(idx)\n",
    "    \n",
    "    path = Path(\"embeddings/\")\n",
    "    if not path.exists():\n",
    "        path.mkdir()\n",
    "        \n",
    "    torch.save(embeddings, f\"embeddings/{num_iter}.pt\")\n",
    "    return embeddings\n",
    "    \n",
    "\n",
    "def calculate_accuracy(pred, yb):\n",
    "    # pred shape: (b, t, v)\n",
    "    _, pred_idx, = pred.max(dim=-1) # (b, t, v) --> (b, t)\n",
    "    \n",
    "    correct = 0\n",
    "    total = pred_idx.size(0)\n",
    "    \n",
    "    # yb shape: (b, t)\n",
    "    for i in range(pred.size(0)):\n",
    "        pred_row = pred_idx[i].tolist()\n",
    "        label_row = yb[i].tolist()\n",
    "        \n",
    "        \n",
    "        # truncate all positions in front with -1\n",
    "        first_index = 0\n",
    "        for i in range(len(label_row)):\n",
    "            if label_row[i] != -1:\n",
    "                first_index = i\n",
    "                break\n",
    "        \n",
    "        stop_index = label_row.index(12)\n",
    "        \n",
    "        pred_row = pred_row[first_index: stop_index]\n",
    "        label_row = label_row[first_index: stop_index]\n",
    "        \n",
    "        if pred_row == label_row:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / total\n",
    "    # truncate all positions in end after (and including) $\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_eval(model, loader):\n",
    "    # calculate loss and also the percentage correct!\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    num_iter = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i, (x,y) in enumerate(loader):\n",
    "        device = next(model.parameters()).device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        out, loss = model(x, targets=y)\n",
    "        acc = calculate_accuracy(out, y)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "        num_iter += 1\n",
    "    \n",
    "    total_loss /= num_iter\n",
    "    total_acc /= num_iter\n",
    "    model.train()\n",
    "    \n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a5d6c-a374-430f-a8fe-c6aabf5ebe6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, iteration 0: Training Loss: 2.9004080295562744\n",
      "Epoch 0, iteration 0: Val loss: 2.692220874130726, val acc: 0.0\n",
      "Epoch 0, iteration 200: Training Loss: 1.951540231704712\n",
      "Epoch 0, iteration 400: Training Loss: 1.9210373163223267\n",
      "Epoch 0, iteration 500: Val loss: 1.8988433666527271, val acc: 0.0\n",
      "Epoch 0, iteration 600: Training Loss: 1.9055200815200806\n",
      "Epoch 1, iteration 0: Training Loss: 1.8539811372756958\n",
      "Epoch 1, iteration 200: Training Loss: 1.9179831743240356\n",
      "Epoch 1, iteration 375: Val loss: 1.805673036724329, val acc: 0.0\n",
      "Epoch 1, iteration 400: Training Loss: 1.8151460886001587\n",
      "Epoch 1, iteration 600: Training Loss: 1.5820730924606323\n",
      "Epoch 2, iteration 0: Training Loss: 1.739691138267517\n",
      "Epoch 2, iteration 200: Training Loss: 1.69881010055542\n",
      "Epoch 2, iteration 250: Val loss: 1.6019847877323627, val acc: 0.0009765625\n",
      "Epoch 2, iteration 400: Training Loss: 1.6268547773361206\n",
      "Epoch 2, iteration 600: Training Loss: 1.5376267433166504\n",
      "Epoch 3, iteration 0: Training Loss: 1.4836920499801636\n",
      "Epoch 3, iteration 125: Val loss: 1.0071545150130987, val acc: 0.0078125\n",
      "Epoch 3, iteration 200: Training Loss: 1.0459991693496704\n",
      "Epoch 3, iteration 400: Training Loss: 0.4849686026573181\n",
      "Epoch 3, iteration 600: Training Loss: 0.5601241588592529\n",
      "Epoch 4, iteration 0: Training Loss: 0.46043193340301514\n",
      "Epoch 4, iteration 0: Val loss: 0.2428202925948426, val acc: 0.6865234375\n",
      "Epoch 4, iteration 200: Training Loss: 0.2836517095565796\n",
      "Epoch 4, iteration 400: Training Loss: 0.31704428791999817\n",
      "Epoch 4, iteration 500: Val loss: 0.12431228545028716, val acc: 0.8271484375\n",
      "Epoch 4, iteration 600: Training Loss: 0.13735930621623993\n",
      "Epoch 5, iteration 0: Training Loss: 0.1692047119140625\n",
      "Epoch 5, iteration 200: Training Loss: 0.24194414913654327\n",
      "Epoch 5, iteration 375: Val loss: 0.05620298534631729, val acc: 0.9189453125\n",
      "Epoch 5, iteration 400: Training Loss: 0.11113882809877396\n",
      "Epoch 5, iteration 600: Training Loss: 0.08005915582180023\n",
      "Epoch 6, iteration 0: Training Loss: 0.038518376648426056\n",
      "Epoch 6, iteration 200: Training Loss: 0.1564180701971054\n",
      "Epoch 6, iteration 250: Val loss: 0.04828340014501009, val acc: 0.92578125\n",
      "Epoch 6, iteration 400: Training Loss: 0.1802089363336563\n",
      "Epoch 6, iteration 600: Training Loss: 0.16438862681388855\n",
      "Epoch 7, iteration 0: Training Loss: 0.05967563018202782\n",
      "Epoch 7, iteration 125: Val loss: 0.038822027683636406, val acc: 0.94140625\n",
      "Epoch 7, iteration 200: Training Loss: 0.13327638804912567\n",
      "Epoch 7, iteration 400: Training Loss: 0.10911475121974945\n",
      "Epoch 7, iteration 600: Training Loss: 0.03647428750991821\n",
      "Epoch 8, iteration 0: Training Loss: 0.04982415959239006\n",
      "Epoch 8, iteration 0: Val loss: 0.02415997134630743, val acc: 0.9482421875\n",
      "Epoch 8, iteration 200: Training Loss: 0.057112205773591995\n",
      "Epoch 8, iteration 400: Training Loss: 0.07310845702886581\n",
      "Epoch 8, iteration 500: Val loss: 0.02086641356254404, val acc: 0.9638671875\n",
      "Epoch 8, iteration 600: Training Loss: 0.11373750865459442\n",
      "Epoch 9, iteration 0: Training Loss: 0.056139376014471054\n",
      "Epoch 9, iteration 200: Training Loss: 0.04207246005535126\n",
      "Epoch 9, iteration 375: Val loss: 0.014205319773282099, val acc: 0.9765625\n",
      "Epoch 9, iteration 400: Training Loss: 0.04691198468208313\n",
      "Epoch 9, iteration 600: Training Loss: 0.02472333423793316\n",
      "Epoch 10, iteration 0: Training Loss: 0.06725787371397018\n",
      "Epoch 10, iteration 200: Training Loss: 0.015102752484381199\n",
      "Epoch 10, iteration 250: Val loss: 0.010612870932163787, val acc: 0.9755859375\n",
      "Epoch 10, iteration 400: Training Loss: 0.002068325411528349\n",
      "Epoch 10, iteration 600: Training Loss: 0.026636451482772827\n",
      "Epoch 11, iteration 0: Training Loss: 0.006378817372024059\n",
      "Epoch 11, iteration 125: Val loss: 0.00999450216841069, val acc: 0.9765625\n",
      "Epoch 11, iteration 200: Training Loss: 0.00972055085003376\n",
      "Epoch 11, iteration 400: Training Loss: 0.010617680847644806\n",
      "Epoch 11, iteration 600: Training Loss: 0.030778391286730766\n",
      "Epoch 12, iteration 0: Training Loss: 0.021573131904006004\n",
      "Epoch 12, iteration 0: Val loss: 0.0059152950834686635, val acc: 0.9833984375\n",
      "Epoch 12, iteration 200: Training Loss: 0.013648614287376404\n",
      "Epoch 12, iteration 400: Training Loss: 0.011179057881236076\n",
      "Epoch 12, iteration 500: Val loss: 0.00463062459300545, val acc: 0.990234375\n",
      "Epoch 12, iteration 600: Training Loss: 0.005794181022793055\n",
      "Epoch 13, iteration 0: Training Loss: 0.01845042034983635\n",
      "Epoch 13, iteration 200: Training Loss: 0.01351328007876873\n",
      "Epoch 13, iteration 375: Val loss: 0.0019453271096381286, val acc: 0.998046875\n",
      "Epoch 13, iteration 400: Training Loss: 0.010878417640924454\n",
      "Epoch 13, iteration 600: Training Loss: 0.011715512722730637\n",
      "Epoch 14, iteration 0: Training Loss: 0.003603793680667877\n",
      "Epoch 14, iteration 200: Training Loss: 0.0019599052611738443\n",
      "Epoch 14, iteration 250: Val loss: 0.002208568706123515, val acc: 0.9951171875\n",
      "Epoch 14, iteration 400: Training Loss: 0.0031266435980796814\n",
      "Epoch 14, iteration 600: Training Loss: 0.0011791469296440482\n",
      "Epoch 15, iteration 0: Training Loss: 0.0034674752969294786\n",
      "Epoch 15, iteration 125: Val loss: 0.0022638182573757604, val acc: 0.994140625\n",
      "Epoch 15, iteration 200: Training Loss: 0.015821432694792747\n",
      "Epoch 15, iteration 400: Training Loss: 0.0077039944007992744\n",
      "Epoch 15, iteration 600: Training Loss: 0.03236628696322441\n",
      "Epoch 16, iteration 0: Training Loss: 0.011238022707402706\n",
      "Epoch 16, iteration 0: Val loss: 0.000990517371064925, val acc: 1.0\n",
      "Epoch 16, iteration 200: Training Loss: 0.017727000638842583\n"
     ]
    }
   ],
   "source": [
    "class GPTConfig:\n",
    "    vocab_size = 14\n",
    "    block_size = 22\n",
    "    n_embd = 64\n",
    "    n_head = 8\n",
    "    n_layer = 6\n",
    "    use_bias = True\n",
    "    dropout = 0.1\n",
    "\n",
    "\n",
    "config = GPTConfig()\n",
    "model = GPT(config)\n",
    "model = model.to('cuda')\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "train_loader = DataLoader(AdditionDataset(), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(AdditionDataset(num_elements=1000), batch_size=32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, 10)\n",
    "training_log = {}\n",
    "\n",
    "training_loop(model, train_loader, val_loader, optimizer, scheduler, training_log, eval_every_iter=500, num_epochs=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8410d4b3-07f7-48a8-9f57-9244fce4bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"adder_gpt_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14c53592-91d2-4364-9d8d-c1abd44bbc59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 21])\n",
      "torch.Size([32, 21])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb , yb = next(iter(train_loader))\n",
    "xb = xb.to('cuda')\n",
    "yb = yb.to('cuda')\n",
    "\n",
    "\n",
    "print(xb.shape)\n",
    "print(yb.shape)\n",
    "\n",
    "logits, loss = model(xb, targets=yb)\n",
    "# print(logits.size())\n",
    "# print(loss)\n",
    "\n",
    "\n",
    "calculate_accuracy(logits, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd87ee-f947-4c39-955b-6d39518c703b",
   "metadata": {},
   "source": [
    "# Messing around with LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f7a04d1-e9b2-4e34-b4d9-a4b1e4261912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.size(): torch.Size([1, 768])\n",
      "answer.size(): torch.Size([1, 768])\n",
      "tensor([-0.0444])\n",
      "tensor([1.0134])\n",
      "tensor([-0.6823,  0.9493, -0.2643, -0.6182, -0.0727,  1.3347, -1.3283,  1.2171,\n",
      "        -1.8012, -0.3760])\n",
      "tensor([-0.6296,  0.9806, -0.2170, -0.5662, -0.0279,  1.3609, -1.2670,  1.2448,\n",
      "        -1.7336, -0.3273])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 768)\n",
    "weight = torch.ones(1, 768)\n",
    "bias = torch.zeros(1, 768)\n",
    "\n",
    "answer = F.layer_norm(a, a.size(), weight, bias)\n",
    "\n",
    "print(f\"a.size(): {a.size()}\")\n",
    "print(f\"answer.size(): {answer.size()}\")\n",
    "\n",
    "a_avg = a.mean(dim=-1)\n",
    "a_std = a.std(dim=-1)\n",
    "\n",
    "print(a_avg)\n",
    "print(a_std)\n",
    "\n",
    "b = (a - a_avg) / a_std\n",
    "\n",
    "print(a[0, 0:10])\n",
    "print(b[0, 0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5309c02-559d-4028-8cfa-124ed98624af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.size(): torch.Size([2, 768])\n",
      "answer.size(): torch.Size([2, 768])\n",
      "tensor([-0.7745, -0.6437,  1.1870,  0.5638, -2.0744,  0.2115,  0.8607,  1.6832,\n",
      "         1.0694,  1.6970]) tensor([-0.7740, -0.6433,  1.1862,  0.5634, -2.0730,  0.2114,  0.8601,  1.6821,\n",
      "         1.0687,  1.6959])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 768)\n",
    "weight = torch.ones(768)\n",
    "bias = torch.zeros(768)\n",
    "\n",
    "answer = F.layer_norm(a, (a.size(-1),), weight, bias)\n",
    "\n",
    "print(f\"a.size(): {a.size()}\")\n",
    "print(f\"answer.size(): {answer.size()}\")\n",
    "\n",
    "a_avg = a.mean(dim=-1, keepdim=True)\n",
    "a_std = a.std(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "# print(f\"a.size(): {a.size()}\")\n",
    "# print(f\"a_avg.size(): {a_avg.size()}\")\n",
    "\n",
    "# a_avg = a_avg.view(a_avg.size(0), -1)\n",
    "# a_std = a_\n",
    "# print(a_avg.size())\n",
    "\n",
    "# print(a_std)\n",
    "\n",
    "b = (a - a_avg.view(a_avg.size(), -1)) / (a_std.view(a_std.size(), -1)**2 + 1e-5)**0.5\n",
    "\n",
    "print(answer[0, :10], b[0, :10])\n",
    "\n",
    "# print(a[0, 0:10])\n",
    "# print(b[0, 0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
