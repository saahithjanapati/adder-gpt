{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eafde01-6d84-4514-9773-888012a03b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e17baa-7d71-452e-8b45-2052f0d671b4",
   "metadata": {},
   "source": [
    "# Vocabulary Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12f80504-350e-4a16-a8b7-7cc6afbf268a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "itos = {\n",
    "    0: \"0\",\n",
    "    1: \"1\",\n",
    "    2: \"2\",\n",
    "    3: \"3\",\n",
    "    4: \"4\",\n",
    "    5: \"5\",\n",
    "    6: \"6\",\n",
    "    7: \"7\",\n",
    "    8: \"8\",\n",
    "    9: \"9\",\n",
    "    10: \"+\",\n",
    "    11: \"=\",\n",
    "    12: \"$\",\n",
    "    13: \".\"\n",
    "}\n",
    "\n",
    "stoi = {v: k for k,v in itos.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9c6dc69-4135-461f-9b20-f83a4ca50ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdditionDataset(Dataset):\n",
    "    def __init__(self, random_seed = 42, max_num_units=6, num_elements = 20000):\n",
    "        self.random_seed = 42\n",
    "        self.num_elements = num_elements\n",
    "        self.max_num_units = max_num_units\n",
    "        self.max_len = max_num_units + 1 + max_num_units + 1 + (max_num_units+1) + 1\n",
    "        \n",
    "        \n",
    "    def select_random_max_val(self):\n",
    "        max_val = 1 * 10**self.max_num_units - 1\n",
    "        \n",
    "        prob = random.random()\n",
    "        \n",
    "        if prob < 0.02:\n",
    "            max_val = 10\n",
    "        \n",
    "        elif prob < 0.06:\n",
    "            max_val = 1e2\n",
    "        \n",
    "        elif prob < 0.11:\n",
    "            max_val = 1e3\n",
    "        \n",
    "        elif prob < 0.16:\n",
    "            max_val = 1e4\n",
    "        \n",
    "        elif prob < 0.20:\n",
    "            max_val = 1e5\n",
    "            \n",
    "        return max_val\n",
    "            \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # randomly select two integers, generate the string and return\n",
    "        a_max_val = self.select_random_max_val()\n",
    "        b_max_val = self.select_random_max_val()\n",
    "        \n",
    "        a = random.randint(0, a_max_val)\n",
    "        b = random.randint(0, b_max_val)\n",
    "        \n",
    "        answer = a + b\n",
    "        \n",
    "        equation_string = str(a) + \"+\" + str(b) + \"=\" + str(answer)[::-1] + \"$\"\n",
    "        while len(equation_string) < self.max_len:\n",
    "            equation_string += \".\"\n",
    "        \n",
    "        # print(equation_string)\n",
    "        \n",
    "        x_list = [stoi[c] for c in equation_string[0:-1]]\n",
    "        y_list = [stoi[c] for c in equation_string[1:]]\n",
    "        \n",
    "        mask_len = len(str(a) + \"+\" + str(b) + \"=\") - 1\n",
    "        \n",
    "        x,y = torch.tensor(x_list), torch.tensor(y_list)\n",
    "        y[:mask_len] = -1\n",
    "        y = torch.where(y == 13,-1, y)\n",
    "        return x, y\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ae4ad18-33ed-4d27-8640-57e4d737ec70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4,  7,  2,  6,  0,  2, 10,  9,  0,  7,  4,  5,  3, 11,  5,  5,  0,  0,\n",
       "          8,  3,  1]),\n",
       " tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  5,  5,  0,  0,  8,\n",
       "          3,  1, 12]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = AdditionDataset()\n",
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73ee0e-3ba5-46b4-93dc-8712fea319fc",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ef62b09-2d9c-4c5c-90a5-51bf9b19ab86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.my_weight = nn.Parameter(torch.ones(config.n_embd))\n",
    "        self.my_bias = nn.Parameter(torch.zeros(config.n_embd)) if config.use_bias else None\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, \n",
    "                     normalized_shape=self.my_weight.shape, \n",
    "                     weight=self.my_weight, \n",
    "                     bias=self.my_bias, \n",
    "                     eps=1e-5)\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.use_bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.use_bias)\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.head_size = int(config.n_embd / config.n_head) # h_dim\n",
    "        self.num_head = config.n_head # number of heads\n",
    "        self.block_size = config.block_size\n",
    "        \n",
    "        self.register_buffer(\"att_mask\", torch.triu(float('-inf') * torch.ones(config.block_size, config.block_size), diagonal=1).view(1, 1, self.block_size, self.block_size))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, t, c = x.size()\n",
    "        \n",
    "        q, k, v = self.c_attn(x).split(c, dim=-1) # (b,t,c)\n",
    "        \n",
    "        q = q.view(b, t, self.num_head, self.head_size).transpose(2,1) # (b, n_h, t, h_dim)\n",
    "        k = k.view(b, t, self.num_head, self.head_size).transpose(2,1) # (b, n_h, t, h_dim)\n",
    "        v = v.view(b, t, self.num_head, self.head_size).transpose(2,1) # (b, n_h, t, h_dim)\n",
    "        \n",
    "        att_scores = q @ k.transpose(-1, -2) / math.sqrt(self.head_size) # (b, n_h, t, h_dim) @ (b, n_h, h_dim, t) --> (b, n_h, t, t)\n",
    "        \n",
    "        # mask the scores\n",
    "        att_scores += self.att_mask[:, :, :t, :t]\n",
    "        \n",
    "        att_scores = torch.softmax(att_scores, dim=-1) # perform softmax for each element\n",
    "        \n",
    "        # perform dropout\n",
    "        att_scores = self.attn_dropout(att_scores) #(b, n_h, t, t)\n",
    "        \n",
    "        out = att_scores @ v # (b, n_h, t, t) @ (b, b_h, t, h_dim) --> (b, n_h, t, h_dim)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous() # (b, t, n_h, h_dim)\n",
    "        out = out.view(b, t, c)\n",
    "        out = self.resid_dropout(self.c_proj(out))\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.up_proj = nn.Linear(config.n_embd, 4*config.n_embd, bias=config.use_bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.down_proj = nn.Linear(config.n_embd * 4, config.n_embd, bias=config.use_bias)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.gelu(self.up_proj(x))\n",
    "        out = self.down_proj(self.dropout(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln_1 = LayerNorm(config)\n",
    "        self.sa = SelfAttention(config)\n",
    "        \n",
    "        self.ln_2 = LayerNorm(config)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x +  self.sa(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.layers = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        \n",
    "        self.ln_f = LayerNorm(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=config.use_bias)\n",
    "        # self.lm_head.weight = self.wte.weight\n",
    "    \n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # make sure x is on same device\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        idx = idx.to(device)\n",
    "        \n",
    "        b, t = idx.size()\n",
    "        \n",
    "        pos = torch.arange(0, t, device=device, dtype=torch.long)\n",
    "        \n",
    "        tok_emb = self.wte(idx) # (b, t, c)\n",
    "        pos_emb = self.wpe(pos) # (t,) --> (t, c)\n",
    "        \n",
    "        x = tok_emb + pos_emb # (b, t, c)\n",
    "        \n",
    "        x = self.drop(x)\n",
    "                \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        \n",
    "        logits = self.lm_head(x) # (b,t, c) --> (b, t, vocab_size)\n",
    "        \n",
    "        loss = None\n",
    "        # compute cross entropy loss if targets are provided\n",
    "        if targets is not None: # targets will be provided in shape b, t\n",
    "            unrolled_targets = targets.view(b*t)\n",
    "            unrolled_logits = logits.view(b*t, -1)\n",
    "            \n",
    "            loss = F.cross_entropy(unrolled_logits, unrolled_targets, ignore_index=-1)        \n",
    "                \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b7613-62ec-45a3-9698-01aa1d3f8473",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abc292b9-8d4a-4abb-a237-dc9637f3686c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_loop(model, train_loader, eval_loader, optimizer, training_log, num_epochs=10, eval_every_iter=1000, print_every=200):\n",
    "    num_iter = 0\n",
    "    \n",
    "    training_log = {\n",
    "        'num_iter': [],\n",
    "        'training_loss': [],\n",
    "        'training_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    \n",
    "    for epoch_num in range(num_epochs):\n",
    "        for i, (x,y) in enumerate(train_loader):\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            outputs, loss = model(x, targets=y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % print_every == 0:\n",
    "                print(f\"Epoch {epoch_num}, iteration {i}: Training Loss: {loss.item()}\")\n",
    "                \n",
    "            if num_iter % eval_every_iter == 0:\n",
    "                \n",
    "                training_log['num_iter'].append(num_iter)\n",
    "                training_log['training_loss'].append(loss.item())\n",
    "                \n",
    "                val_loss, val_acc = run_eval(model, eval_loader)\n",
    "                print(f\"Epoch {epoch_num}, iteration {i}: Val loss: {val_loss}, val acc: {val_acc}\")\n",
    "                train_acc = calculate_accuracy(outputs, y)\n",
    "                \n",
    "                training_log['training_acc'].append(train_acc)\n",
    "                training_log['val_loss'].append(val_loss)\n",
    "                training_log['val_acc'].append(val_acc)\n",
    "\n",
    "            num_iter += 1\n",
    "    \n",
    "    return training_log\n",
    "\n",
    "            \n",
    "\n",
    "def calculate_accuracy(pred, yb):\n",
    "    # pred shape: (b, t, v)\n",
    "    _, pred_idx, = pred.max(dim=-1) # (b, t, v) --> (b, t)\n",
    "    \n",
    "    correct = 0\n",
    "    total = pred_idx.size(0)\n",
    "    \n",
    "    # yb shape: (b, t)\n",
    "    for i in range(pred.size(0)):\n",
    "        pred_row = pred_idx[i].tolist()\n",
    "        label_row = yb[i].tolist()\n",
    "        \n",
    "        \n",
    "        # truncate all positions in front with -1\n",
    "        first_index = 0\n",
    "        for i in range(len(label_row)):\n",
    "            if label_row[i] != -1:\n",
    "                first_index = i\n",
    "                break\n",
    "        \n",
    "        stop_index = label_row.index(12)\n",
    "        \n",
    "        pred_row = pred_row[first_index: stop_index]\n",
    "        label_row = label_row[first_index: stop_index]\n",
    "        \n",
    "        if pred_row == label_row:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / total\n",
    "    # truncate all positions in end after (and including) $\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_eval(model, loader):\n",
    "    # calculate loss and also the percentage correct!\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    num_iter = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i, (x,y) in enumerate(loader):\n",
    "        device = next(model.parameters()).device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        out, loss = model(x, targets=y)\n",
    "        acc = calculate_accuracy(out, y)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "        num_iter += 1\n",
    "    \n",
    "    total_loss /= num_iter\n",
    "    total_acc /= num_iter\n",
    "    model.train()\n",
    "    \n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "342a5d6c-a374-430f-a8fe-c6aabf5ebe6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, iteration 0: Training Loss: 3.0833637714385986\n",
      "Epoch 0, iteration 0: Val loss: 2.689970925450325, val acc: 0.0\n",
      "Epoch 0, iteration 200: Training Loss: 1.9339945316314697\n",
      "Epoch 0, iteration 400: Training Loss: 1.8367029428482056\n",
      "Epoch 0, iteration 500: Val loss: 1.8933150619268417, val acc: 0.0\n",
      "Epoch 0, iteration 600: Training Loss: 1.906567096710205\n",
      "Epoch 1, iteration 0: Training Loss: 1.8573038578033447\n",
      "Epoch 1, iteration 200: Training Loss: 1.8058016300201416\n",
      "Epoch 1, iteration 375: Val loss: 1.6979985162615776, val acc: 0.0009765625\n",
      "Epoch 1, iteration 400: Training Loss: 1.6868118047714233\n",
      "Epoch 1, iteration 600: Training Loss: 1.7411812543869019\n",
      "Epoch 2, iteration 0: Training Loss: 1.7403244972229004\n",
      "Epoch 2, iteration 200: Training Loss: 1.638872504234314\n",
      "Epoch 2, iteration 250: Val loss: 1.6298180744051933, val acc: 0.0\n",
      "Epoch 2, iteration 400: Training Loss: 1.6332590579986572\n",
      "Epoch 2, iteration 600: Training Loss: 1.4790061712265015\n",
      "Epoch 3, iteration 0: Training Loss: 1.4555482864379883\n",
      "Epoch 3, iteration 125: Val loss: 0.9208882544189692, val acc: 0.037109375\n",
      "Epoch 3, iteration 200: Training Loss: 0.8317099213600159\n",
      "Epoch 3, iteration 400: Training Loss: 0.4829166531562805\n",
      "Epoch 3, iteration 600: Training Loss: 0.2533220946788788\n",
      "Epoch 4, iteration 0: Training Loss: 0.39788195490837097\n",
      "Epoch 4, iteration 0: Val loss: 0.1265037745470181, val acc: 0.8271484375\n",
      "Epoch 4, iteration 200: Training Loss: 0.1791413575410843\n",
      "Epoch 4, iteration 400: Training Loss: 0.1645043045282364\n",
      "Epoch 4, iteration 500: Val loss: 0.04530063238780713, val acc: 0.9384765625\n",
      "Epoch 4, iteration 600: Training Loss: 0.15327051281929016\n",
      "Epoch 5, iteration 0: Training Loss: 0.06866085529327393\n",
      "Epoch 5, iteration 200: Training Loss: 0.10840346664190292\n",
      "Epoch 5, iteration 375: Val loss: 0.030531067226547748, val acc: 0.953125\n",
      "Epoch 5, iteration 400: Training Loss: 0.03521299734711647\n",
      "Epoch 5, iteration 600: Training Loss: 0.05382025986909866\n",
      "Epoch 6, iteration 0: Training Loss: 0.06203100085258484\n",
      "Epoch 6, iteration 200: Training Loss: 0.12260149419307709\n",
      "Epoch 6, iteration 250: Val loss: 0.017370868480611534, val acc: 0.966796875\n",
      "Epoch 6, iteration 400: Training Loss: 0.19174529612064362\n",
      "Epoch 6, iteration 600: Training Loss: 0.02868998609483242\n",
      "Epoch 7, iteration 0: Training Loss: 0.03022019937634468\n",
      "Epoch 7, iteration 125: Val loss: 0.009137761789133947, val acc: 0.9853515625\n",
      "Epoch 7, iteration 200: Training Loss: 0.06029356271028519\n",
      "Epoch 7, iteration 400: Training Loss: 0.02368088811635971\n",
      "Epoch 7, iteration 600: Training Loss: 0.06838030368089676\n",
      "Epoch 8, iteration 0: Training Loss: 0.04676114395260811\n",
      "Epoch 8, iteration 0: Val loss: 0.007507554583753517, val acc: 0.9833984375\n",
      "Epoch 8, iteration 200: Training Loss: 0.009738600812852383\n",
      "Epoch 8, iteration 400: Training Loss: 0.13881705701351166\n",
      "Epoch 8, iteration 500: Val loss: 0.008264728359790752, val acc: 0.984375\n",
      "Epoch 8, iteration 600: Training Loss: 0.060466546565294266\n",
      "Epoch 9, iteration 0: Training Loss: 0.07329019159078598\n",
      "Epoch 9, iteration 200: Training Loss: 0.03218260407447815\n",
      "Epoch 9, iteration 375: Val loss: 0.0018506547430661158, val acc: 0.998046875\n",
      "Epoch 9, iteration 400: Training Loss: 0.018357425928115845\n",
      "Epoch 9, iteration 600: Training Loss: 0.007912532426416874\n",
      "Epoch 10, iteration 0: Training Loss: 0.11058813333511353\n",
      "Epoch 10, iteration 200: Training Loss: 0.017093241214752197\n",
      "Epoch 10, iteration 250: Val loss: 0.0009773711612979241, val acc: 0.9990234375\n",
      "Epoch 10, iteration 400: Training Loss: 0.0401557981967926\n",
      "Epoch 10, iteration 600: Training Loss: 0.05172164365649223\n",
      "Epoch 11, iteration 0: Training Loss: 0.018534483388066292\n",
      "Epoch 11, iteration 125: Val loss: 0.0024767099841938034, val acc: 0.9951171875\n",
      "Epoch 11, iteration 200: Training Loss: 0.007568670436739922\n",
      "Epoch 11, iteration 400: Training Loss: 0.02431534416973591\n",
      "Epoch 11, iteration 600: Training Loss: 0.040917154401540756\n",
      "Epoch 12, iteration 0: Training Loss: 0.012900757603347301\n",
      "Epoch 12, iteration 0: Val loss: 0.00095782069206507, val acc: 0.998046875\n",
      "Epoch 12, iteration 200: Training Loss: 0.002065518870949745\n",
      "Epoch 12, iteration 400: Training Loss: 0.019047176465392113\n",
      "Epoch 12, iteration 500: Val loss: 0.001228936302254624, val acc: 0.9970703125\n",
      "Epoch 12, iteration 600: Training Loss: 0.014026972465217113\n",
      "Epoch 13, iteration 0: Training Loss: 0.04501345381140709\n",
      "Epoch 13, iteration 200: Training Loss: 0.025039535015821457\n",
      "Epoch 13, iteration 375: Val loss: 0.003932393459308514, val acc: 0.994140625\n",
      "Epoch 13, iteration 400: Training Loss: 0.018974855542182922\n",
      "Epoch 13, iteration 600: Training Loss: 0.0469963513314724\n",
      "Epoch 14, iteration 0: Training Loss: 0.04230349138379097\n",
      "Epoch 14, iteration 200: Training Loss: 0.03934156894683838\n",
      "Epoch 14, iteration 250: Val loss: 0.0008945225602019491, val acc: 0.998046875\n",
      "Epoch 14, iteration 400: Training Loss: 0.00256644980981946\n",
      "Epoch 14, iteration 600: Training Loss: 0.011788138188421726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_iter': [0,\n",
       "  500,\n",
       "  1000,\n",
       "  1500,\n",
       "  2000,\n",
       "  2500,\n",
       "  3000,\n",
       "  3500,\n",
       "  4000,\n",
       "  4500,\n",
       "  5000,\n",
       "  5500,\n",
       "  6000,\n",
       "  6500,\n",
       "  7000,\n",
       "  7500,\n",
       "  8000,\n",
       "  8500,\n",
       "  9000],\n",
       " 'training_loss': [3.0833637714385986,\n",
       "  1.9162452220916748,\n",
       "  1.7080167531967163,\n",
       "  1.5419331789016724,\n",
       "  1.1010147333145142,\n",
       "  0.39788195490837097,\n",
       "  0.12680713832378387,\n",
       "  0.0949561819434166,\n",
       "  0.09352259337902069,\n",
       "  0.0313975028693676,\n",
       "  0.04676114395260811,\n",
       "  0.12011094391345978,\n",
       "  0.02751099318265915,\n",
       "  0.03308473899960518,\n",
       "  0.030764304101467133,\n",
       "  0.012900757603347301,\n",
       "  0.008312731981277466,\n",
       "  0.10735060274600983,\n",
       "  0.050702426582574844],\n",
       " 'training_acc': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.03125,\n",
       "  0.4375,\n",
       "  0.78125,\n",
       "  0.84375,\n",
       "  0.875,\n",
       "  0.9375,\n",
       "  0.90625,\n",
       "  0.78125,\n",
       "  0.90625,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  0.875,\n",
       "  0.9375],\n",
       " 'val_loss': [2.689970925450325,\n",
       "  1.8933150619268417,\n",
       "  1.6979985162615776,\n",
       "  1.6298180744051933,\n",
       "  0.9208882544189692,\n",
       "  0.1265037745470181,\n",
       "  0.04530063238780713,\n",
       "  0.030531067226547748,\n",
       "  0.017370868480611534,\n",
       "  0.009137761789133947,\n",
       "  0.007507554583753517,\n",
       "  0.008264728359790752,\n",
       "  0.0018506547430661158,\n",
       "  0.0009773711612979241,\n",
       "  0.0024767099841938034,\n",
       "  0.00095782069206507,\n",
       "  0.001228936302254624,\n",
       "  0.003932393459308514,\n",
       "  0.0008945225602019491],\n",
       " 'val_acc': [0.0,\n",
       "  0.0,\n",
       "  0.0009765625,\n",
       "  0.0,\n",
       "  0.037109375,\n",
       "  0.8271484375,\n",
       "  0.9384765625,\n",
       "  0.953125,\n",
       "  0.966796875,\n",
       "  0.9853515625,\n",
       "  0.9833984375,\n",
       "  0.984375,\n",
       "  0.998046875,\n",
       "  0.9990234375,\n",
       "  0.9951171875,\n",
       "  0.998046875,\n",
       "  0.9970703125,\n",
       "  0.994140625,\n",
       "  0.998046875]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPTConfig:\n",
    "    vocab_size = 14\n",
    "    block_size = 22\n",
    "    n_embd = 64\n",
    "    n_head = 8\n",
    "    n_layer = 6\n",
    "    use_bias = True\n",
    "    dropout = 0.1\n",
    "\n",
    "\n",
    "config = GPTConfig()\n",
    "model = GPT(config)\n",
    "model = model.to('cuda')\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(AdditionDataset(), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(AdditionDataset(num_elements=1000), batch_size=32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, )\n",
    "training_log = {}\n",
    "\n",
    "training_loop(model, train_loader, val_loader, optimizer, training_log, eval_every_iter=500, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14c53592-91d2-4364-9d8d-c1abd44bbc59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 21])\n",
      "torch.Size([32, 21])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb , yb = next(iter(train_loader))\n",
    "xb = xb.to('cuda')\n",
    "yb = yb.to('cuda')\n",
    "\n",
    "\n",
    "print(xb.shape)\n",
    "print(yb.shape)\n",
    "\n",
    "logits, loss = model(xb, targets=yb)\n",
    "# print(logits.size())\n",
    "# print(loss)\n",
    "\n",
    "\n",
    "calculate_accuracy(logits, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd87ee-f947-4c39-955b-6d39518c703b",
   "metadata": {},
   "source": [
    "# Messing around with LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f7a04d1-e9b2-4e34-b4d9-a4b1e4261912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.size(): torch.Size([1, 768])\n",
      "answer.size(): torch.Size([1, 768])\n",
      "tensor([-0.0444])\n",
      "tensor([1.0134])\n",
      "tensor([-0.6823,  0.9493, -0.2643, -0.6182, -0.0727,  1.3347, -1.3283,  1.2171,\n",
      "        -1.8012, -0.3760])\n",
      "tensor([-0.6296,  0.9806, -0.2170, -0.5662, -0.0279,  1.3609, -1.2670,  1.2448,\n",
      "        -1.7336, -0.3273])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 768)\n",
    "weight = torch.ones(1, 768)\n",
    "bias = torch.zeros(1, 768)\n",
    "\n",
    "answer = F.layer_norm(a, a.size(), weight, bias)\n",
    "\n",
    "print(f\"a.size(): {a.size()}\")\n",
    "print(f\"answer.size(): {answer.size()}\")\n",
    "\n",
    "a_avg = a.mean(dim=-1)\n",
    "a_std = a.std(dim=-1)\n",
    "\n",
    "print(a_avg)\n",
    "print(a_std)\n",
    "\n",
    "b = (a - a_avg) / a_std\n",
    "\n",
    "print(a[0, 0:10])\n",
    "print(b[0, 0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5309c02-559d-4028-8cfa-124ed98624af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.size(): torch.Size([2, 768])\n",
      "answer.size(): torch.Size([2, 768])\n",
      "tensor([-0.7745, -0.6437,  1.1870,  0.5638, -2.0744,  0.2115,  0.8607,  1.6832,\n",
      "         1.0694,  1.6970]) tensor([-0.7740, -0.6433,  1.1862,  0.5634, -2.0730,  0.2114,  0.8601,  1.6821,\n",
      "         1.0687,  1.6959])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 768)\n",
    "weight = torch.ones(768)\n",
    "bias = torch.zeros(768)\n",
    "\n",
    "answer = F.layer_norm(a, (a.size(-1),), weight, bias)\n",
    "\n",
    "print(f\"a.size(): {a.size()}\")\n",
    "print(f\"answer.size(): {answer.size()}\")\n",
    "\n",
    "a_avg = a.mean(dim=-1, keepdim=True)\n",
    "a_std = a.std(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "# print(f\"a.size(): {a.size()}\")\n",
    "# print(f\"a_avg.size(): {a_avg.size()}\")\n",
    "\n",
    "# a_avg = a_avg.view(a_avg.size(0), -1)\n",
    "# a_std = a_\n",
    "# print(a_avg.size())\n",
    "\n",
    "# print(a_std)\n",
    "\n",
    "b = (a - a_avg.view(a_avg.size(), -1)) / (a_std.view(a_std.size(), -1)**2 + 1e-5)**0.5\n",
    "\n",
    "print(answer[0, :10], b[0, :10])\n",
    "\n",
    "# print(a[0, 0:10])\n",
    "# print(b[0, 0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
